{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
      "License(s): CC0-1.0\n",
      "amazon-books-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"aidanaakkaziyeva\"\n",
    "os.environ['KAGGLE_KEY'] = \"609e0e320a0900d9d1865319a498c843\"\n",
    "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Define the zip file name\n",
    "zip_file = \"amazon-books-reviews.zip\"\n",
    "\n",
    "# Extract all contents\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"amazon-books-reviews\")  # Extracts into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books_data.csv', 'Books_rating.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"amazon-books-reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Books Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = pd.read_csv(\"amazon-books-reviews/books_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>image</th>\n",
       "      <th>previewLink</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publishedDate</th>\n",
       "      <th>infoLink</th>\n",
       "      <th>categories</th>\n",
       "      <th>ratingsCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Julie Strain']</td>\n",
       "      <td>http://books.google.com/books/content?id=DykPA...</td>\n",
       "      <td>http://books.google.nl/books?id=DykPAAAACAAJ&amp;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>http://books.google.nl/books?id=DykPAAAACAAJ&amp;d...</td>\n",
       "      <td>['Comics &amp; Graphic Novels']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>Philip Nel takes a fascinating look into the k...</td>\n",
       "      <td>['Philip Nel']</td>\n",
       "      <td>http://books.google.com/books/content?id=IjvHQ...</td>\n",
       "      <td>http://books.google.nl/books?id=IjvHQsCn_pgC&amp;p...</td>\n",
       "      <td>A&amp;C Black</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>http://books.google.nl/books?id=IjvHQsCn_pgC&amp;d...</td>\n",
       "      <td>['Biography &amp; Autobiography']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>This resource includes twelve principles in un...</td>\n",
       "      <td>['David R. Ray']</td>\n",
       "      <td>http://books.google.com/books/content?id=2tsDA...</td>\n",
       "      <td>http://books.google.nl/books?id=2tsDAAAACAAJ&amp;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>http://books.google.nl/books?id=2tsDAAAACAAJ&amp;d...</td>\n",
       "      <td>['Religion']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>Julia Thomas finds her life spinning out of co...</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>http://books.google.com/books/content?id=aRSIg...</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>iUniverse</td>\n",
       "      <td>2005-02</td>\n",
       "      <td>http://books.google.nl/books?id=aRSIgJlq6JwC&amp;d...</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nation Dance: Religion, Identity and Cultural ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Edward Long']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://books.google.nl/books?id=399SPgAACAAJ&amp;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003-03-01</td>\n",
       "      <td>http://books.google.nl/books?id=399SPgAACAAJ&amp;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                     Its Only Art If Its Well Hung!   \n",
       "1                           Dr. Seuss: American Icon   \n",
       "2              Wonderful Worship in Smaller Churches   \n",
       "3                      Whispers of the Wicked Saints   \n",
       "4  Nation Dance: Religion, Identity and Cultural ...   \n",
       "\n",
       "                                         description              authors  \\\n",
       "0                                                NaN     ['Julie Strain']   \n",
       "1  Philip Nel takes a fascinating look into the k...       ['Philip Nel']   \n",
       "2  This resource includes twelve principles in un...     ['David R. Ray']   \n",
       "3  Julia Thomas finds her life spinning out of co...  ['Veronica Haddon']   \n",
       "4                                                NaN      ['Edward Long']   \n",
       "\n",
       "                                               image  \\\n",
       "0  http://books.google.com/books/content?id=DykPA...   \n",
       "1  http://books.google.com/books/content?id=IjvHQ...   \n",
       "2  http://books.google.com/books/content?id=2tsDA...   \n",
       "3  http://books.google.com/books/content?id=aRSIg...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         previewLink  publisher publishedDate  \\\n",
       "0  http://books.google.nl/books?id=DykPAAAACAAJ&d...        NaN          1996   \n",
       "1  http://books.google.nl/books?id=IjvHQsCn_pgC&p...  A&C Black    2005-01-01   \n",
       "2  http://books.google.nl/books?id=2tsDAAAACAAJ&d...        NaN          2000   \n",
       "3  http://books.google.nl/books?id=aRSIgJlq6JwC&d...  iUniverse       2005-02   \n",
       "4  http://books.google.nl/books?id=399SPgAACAAJ&d...        NaN    2003-03-01   \n",
       "\n",
       "                                            infoLink  \\\n",
       "0  http://books.google.nl/books?id=DykPAAAACAAJ&d...   \n",
       "1  http://books.google.nl/books?id=IjvHQsCn_pgC&d...   \n",
       "2  http://books.google.nl/books?id=2tsDAAAACAAJ&d...   \n",
       "3  http://books.google.nl/books?id=aRSIgJlq6JwC&d...   \n",
       "4  http://books.google.nl/books?id=399SPgAACAAJ&d...   \n",
       "\n",
       "                      categories  ratingsCount  \n",
       "0    ['Comics & Graphic Novels']           NaN  \n",
       "1  ['Biography & Autobiography']           NaN  \n",
       "2                   ['Religion']           NaN  \n",
       "3                    ['Fiction']           NaN  \n",
       "4                            NaN           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212404, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing the Books Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>categories</th>\n",
       "      <th>authors</th>\n",
       "      <th>publishedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>['Comics &amp; Graphic Novels']</td>\n",
       "      <td>['Julie Strain']</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>['Biography &amp; Autobiography']</td>\n",
       "      <td>['Philip Nel']</td>\n",
       "      <td>2005-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>['Religion']</td>\n",
       "      <td>['David R. Ray']</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>['Veronica Haddon']</td>\n",
       "      <td>2005-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nation Dance: Religion, Identity and Cultural ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Edward Long']</td>\n",
       "      <td>2003-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                     Its Only Art If Its Well Hung!   \n",
       "1                           Dr. Seuss: American Icon   \n",
       "2              Wonderful Worship in Smaller Churches   \n",
       "3                      Whispers of the Wicked Saints   \n",
       "4  Nation Dance: Religion, Identity and Cultural ...   \n",
       "\n",
       "                      categories              authors publishedDate  \n",
       "0    ['Comics & Graphic Novels']     ['Julie Strain']          1996  \n",
       "1  ['Biography & Autobiography']       ['Philip Nel']    2005-01-01  \n",
       "2                   ['Religion']     ['David R. Ray']          2000  \n",
       "3                    ['Fiction']  ['Veronica Haddon']       2005-02  \n",
       "4                            NaN      ['Edward Long']    2003-03-01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only columns 'Title', 'categories'\n",
    "books_data = books_data[['Title', 'categories', 'authors', 'publishedDate']]\n",
    "books_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10883"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show unique values in the column categories\n",
    "books_data['categories'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories\n",
       "['Fiction']                        23419\n",
       "['Religion']                        9459\n",
       "['History']                         9330\n",
       "['Juvenile Fiction']                6643\n",
       "['Biography & Autobiography']       6324\n",
       "['Business & Economics']            5625\n",
       "['Computers']                       4312\n",
       "['Social Science']                  3834\n",
       "['Juvenile Nonfiction']             3446\n",
       "['Science']                         2623\n",
       "['Education']                       2611\n",
       "['Cooking']                         2445\n",
       "['Sports & Recreation']             2267\n",
       "['Family & Relationships']          2178\n",
       "['Literary Criticism']              2147\n",
       "['Music']                           2106\n",
       "['Medical']                         2079\n",
       "['Art']                             2054\n",
       "['Body, Mind & Spirit']             2049\n",
       "['Language Arts & Disciplines']     2036\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top 20 categories\n",
    "books_data['categories'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.loc[:, 'categories'] = books_data['categories'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Cooking Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to include only rows where the categories column contains 'Cooking' (and not missing)\n",
    "df_cooking = books_data[\n",
    "    books_data['categories'].apply(\n",
    "        lambda x: isinstance(x, list) and any(cat.strip().lower() == 'cooking' for cat in x)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>categories</th>\n",
       "      <th>authors</th>\n",
       "      <th>publishedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "      <td>[Cooking]</td>\n",
       "      <td>['Ruth Allman']</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Old-Fashioned Ckbk</td>\n",
       "      <td>[Cooking]</td>\n",
       "      <td>['Don Holm']</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Basil: A Book of Recipes (The Little Recipe Bo...</td>\n",
       "      <td>[Cooking]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Vino Para Dummies</td>\n",
       "      <td>[Cooking]</td>\n",
       "      <td>['Ed McCarthy', 'Mary Ewing-Mulligan']</td>\n",
       "      <td>2011-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>Flavors of Korea: Delicious Vegetarian Cuisine...</td>\n",
       "      <td>[Cooking]</td>\n",
       "      <td>['Deborah Coultrip-Davis', 'Young Sook Ramsay']</td>\n",
       "      <td>1998-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title categories  \\\n",
       "19                                    Alaska Sourdough  [Cooking]   \n",
       "199                                 Old-Fashioned Ckbk  [Cooking]   \n",
       "280  Basil: A Book of Recipes (The Little Recipe Bo...  [Cooking]   \n",
       "351                                  Vino Para Dummies  [Cooking]   \n",
       "423  Flavors of Korea: Delicious Vegetarian Cuisine...  [Cooking]   \n",
       "\n",
       "                                             authors publishedDate  \n",
       "19                                   ['Ruth Allman']          1976  \n",
       "199                                     ['Don Holm']          1969  \n",
       "280                                              NaN          1997  \n",
       "351           ['Ed McCarthy', 'Mary Ewing-Mulligan']    2011-03-03  \n",
       "423  ['Deborah Coultrip-Davis', 'Young Sook Ramsay']    1998-01-01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2452, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with the same values for authors and publishedDate\n",
    "df_cooking = df_cooking.drop_duplicates(subset=['authors', 'publishedDate'])\n",
    "\n",
    "# remove rows with the same values for title\n",
    "df_cooking = df_cooking.drop_duplicates(subset=['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2319, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only column Title \n",
    "df_cooking = df_cooking[['Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Old-Fashioned Ckbk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Basil: A Book of Recipes (The Little Recipe Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Vino Para Dummies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>Flavors of Korea: Delicious Vegetarian Cuisine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title\n",
       "19                                    Alaska Sourdough\n",
       "199                                 Old-Fashioned Ckbk\n",
       "280  Basil: A Book of Recipes (The Little Recipe Bo...\n",
       "351                                  Vino Para Dummies\n",
       "423  Flavors of Korea: Delicious Vegetarian Cuisine..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Books Rating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_rating = pd.read_csv(\"amazon-books-reviews/Books_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>User_id</th>\n",
       "      <th>profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1882931173</td>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>940636800</td>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>Kevin Killian</td>\n",
       "      <td>10/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1095724800</td>\n",
       "      <td>Really Enjoyed It</td>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>John Granger</td>\n",
       "      <td>10/11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1078790400</td>\n",
       "      <td>Essential for every personal and Public Library</td>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>Roy E. Perry \"amateur philosopher\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1090713600</td>\n",
       "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>D. H. Richards \"ninthwavestore\"</td>\n",
       "      <td>3/3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1107993600</td>\n",
       "      <td>Good academic overview</td>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                           Title  Price         User_id  \\\n",
       "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
       "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
       "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
       "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
       "4  0826414346        Dr. Seuss: American Icon    NaN  A22X4XUPKF66MR   \n",
       "\n",
       "                          profileName review/helpfulness  review/score  \\\n",
       "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
       "1                       Kevin Killian              10/10           5.0   \n",
       "2                        John Granger              10/11           5.0   \n",
       "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
       "4     D. H. Richards \"ninthwavestore\"                3/3           4.0   \n",
       "\n",
       "   review/time                                   review/summary  \\\n",
       "0    940636800           Nice collection of Julie Strain images   \n",
       "1   1095724800                                Really Enjoyed It   \n",
       "2   1078790400  Essential for every personal and Public Library   \n",
       "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
       "4   1107993600                           Good academic overview   \n",
       "\n",
       "                                         review/text  \n",
       "0  This is only for Julie Strain fans. It's a col...  \n",
       "1  I don't care much for Dr. Seuss but after read...  \n",
       "2  If people become the books they read and if \"t...  \n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  \n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>review/text</th>\n",
       "      <th>User_id</th>\n",
       "      <th>review/time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>940636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>1095724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>1078790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>1090713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>1107993600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title  \\\n",
       "0  Its Only Art If Its Well Hung!   \n",
       "1        Dr. Seuss: American Icon   \n",
       "2        Dr. Seuss: American Icon   \n",
       "3        Dr. Seuss: American Icon   \n",
       "4        Dr. Seuss: American Icon   \n",
       "\n",
       "                                         review/text         User_id  \\\n",
       "0  This is only for Julie Strain fans. It's a col...   AVCGYZL8FQQTD   \n",
       "1  I don't care much for Dr. Seuss but after read...  A30TK6U7DNS82R   \n",
       "2  If people become the books they read and if \"t...  A3UH4UZ4RSVO82   \n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  A2MVUWT453QH61   \n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...  A22X4XUPKF66MR   \n",
       "\n",
       "   review/time  \n",
       "0    940636800  \n",
       "1   1095724800  \n",
       "2   1078790400  \n",
       "3   1090713600  \n",
       "4   1107993600  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the columns that are not needed (Price, review/time, review/summary, profileName, review/text, review/helpfulness)\n",
    "books_rating = books_rating[['Title', 'review/text', 'User_id', 'review/time']]\n",
    "books_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1548183, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows with the same unique values for User_id and review/time\n",
    "books_rating = books_rating.drop_duplicates(subset=['User_id', 'review/time']) #unix timestamp\n",
    "books_rating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column review/time\n",
    "books_rating = books_rating.drop(columns=['review/time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title            17\n",
       "review/text       1\n",
       "User_id        5558\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display null values in the dataset\n",
    "books_rating.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values \n",
    "books_rating = books_rating.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542607, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_rating.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Cooking Books with Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>review/text</th>\n",
       "      <th>User_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "      <td>I have been using this book since 1988, the ei...</td>\n",
       "      <td>AC58Z72OB2DDX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "      <td>My poor dogeared, stained copy of this book ca...</td>\n",
       "      <td>A3CNQIKVTG9QYO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "      <td>As a former Alaskan, I didn't want to have to ...</td>\n",
       "      <td>A2UMP9TJTJ6A6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "      <td>For those of us who would prefer to use sourdo...</td>\n",
       "      <td>AC2TK7NHKB5C0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Sourdough</td>\n",
       "      <td>Make the most sublime waffles - crispy outside...</td>\n",
       "      <td>A22T74YNRM8NTK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Title                                        review/text  \\\n",
       "0  Alaska Sourdough  I have been using this book since 1988, the ei...   \n",
       "1  Alaska Sourdough  My poor dogeared, stained copy of this book ca...   \n",
       "2  Alaska Sourdough  As a former Alaskan, I didn't want to have to ...   \n",
       "3  Alaska Sourdough  For those of us who would prefer to use sourdo...   \n",
       "4  Alaska Sourdough  Make the most sublime waffles - crispy outside...   \n",
       "\n",
       "          User_id  \n",
       "0   AC58Z72OB2DDX  \n",
       "1  A3CNQIKVTG9QYO  \n",
       "2  A2UMP9TJTJ6A6B  \n",
       "3   AC2TK7NHKB5C0  \n",
       "4  A22T74YNRM8NTK  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match the books_rating data with the df_romance data using the Title column\n",
    "\n",
    "df_cooking = df_cooking.merge(books_rating, on='Title', how='inner')\n",
    "df_cooking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22434, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where User_id is not all caps and that has spaces\n",
    "df_cooking = df_cooking[df_cooking['User_id'].str.isupper() & ~df_cooking['User_id'].str.contains(' ')]\n",
    "df_cooking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title          0\n",
       "review/text    0\n",
       "User_id        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values \n",
    "df_cooking.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "The Bread Lover's Bread Machine Cookbook: A Master Baker's 300 Favorite Recipes for Perfect-Every-Time Bread-From Every Kind of Machine      398\n",
       "Bread Baker's Apprentice                                                                                                                     358\n",
       "How To Cook Everything: Simple Recipes for Great Food                                                                                        293\n",
       "Eating For Life                                                                                                                              263\n",
       "Saving Dinner: The Menus, Recipes, and Shopping Lists to Bring Your Family Back to the Table                                                 250\n",
       "                                                                                                                                            ... \n",
       "The Co-ed cookbook (Scholastic starline)                                                                                                       1\n",
       "Cooking Vegetables                                                                                                                             1\n",
       "Cooking for Heart and Soul: 100 Delicious Low-Fat Recipes from San Francisco's Top ChefsA Cookbook to Benefit the San Francisco Food Bank      1\n",
       "The art of Irish cooking                                                                                                                       1\n",
       "Old-Fashioned Ckbk                                                                                                                             1\n",
       "Name: count, Length: 2230, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count records per book \n",
    "df_cooking['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(112.57805117232772)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average length of the reviews\n",
    "df_cooking['review/text'].apply(lambda x: len(str(x).split())).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.ml.feature import MinHashLSH, HashingTF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import lower, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BooksReviewSimilarity\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the DataFrame from pandas to Spark\n",
    "df_cooking = spark.createDataFrame(df_cooking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------+\n",
      "|           Title|         review/text|       User_id|\n",
      "+----------------+--------------------+--------------+\n",
      "|Alaska Sourdough|I have been using...| AC58Z72OB2DDX|\n",
      "|Alaska Sourdough|My poor dogeared,...|A3CNQIKVTG9QYO|\n",
      "|Alaska Sourdough|As a former Alask...|A2UMP9TJTJ6A6B|\n",
      "|Alaska Sourdough|For those of us w...| AC2TK7NHKB5C0|\n",
      "|Alaska Sourdough|Make the most sub...|A22T74YNRM8NTK|\n",
      "+----------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cooking.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22434, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11217, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample the dataset (50%)\n",
    "# df_cooking = df_cooking.sample(0.5, seed=123)\n",
    "# df_cooking.count(), len(df_cooking.columns)\n",
    "\n",
    "\n",
    "total_rows = df_cooking.count()\n",
    "\n",
    "# Calculate the number of rows for the first half\n",
    "half_rows = total_rows // 2  \n",
    "\n",
    "# Subset the first half of the dataset\n",
    "df_cooking = df_cooking.limit(half_rows)\n",
    "\n",
    "df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, upper\n",
    "\n",
    "# # drop rows where User_id is not all caps and that has spaces\n",
    "# df_cooking = df_cooking.filter(\n",
    "# \t(col(\"User_id\") == upper(col(\"User_id\"))) & (~col(\"User_id\").contains(\" \"))\n",
    "# )\n",
    "# df_cooking.count(), len(df_cooking.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that a user only has one entry per book\n",
    "df_cooking = df_cooking.dropDuplicates(\n",
    "    subset=[\"Title\", \"User_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_cooking.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(lower(col(\"review/text\")), r'[^\\w\\s]', '')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|\n",
      "+--------------------+--------------------+--------------+--------------------+\n",
      "|1,000 Vegetarian ...|This book is one ...|A12KSAYBWRE2EP|this book is one ...|\n",
      "|1,000 Vegetarian ...|I have used this ...|A14O63Q1W61YO5|i have used this ...|\n",
      "|1,000 Vegetarian ...|In our house, we ...|A17FLJJOOO0M9V|in our house we h...|\n",
      "|1,000 Vegetarian ...|I am pleasantly s...|A1BBJK1VE07KCV|i am pleasantly s...|\n",
      "|1,000 Vegetarian ...|I own just about ...|A1FMR10ARQHGKF|i own just about ...|\n",
      "+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define custom stopwords\n",
    "custom_stopwords = [\"book\", \"read\", \"recipe\", \"good\", \"great\"]\n",
    "\n",
    "# Combine with default stopwords\n",
    "default_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "all_stopwords = default_stopwords + custom_stopwords\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_text\", stopWords=all_stopwords)\n",
    "df_no_stop = remover.transform(df_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1,000 Vegetarian ...|This book is one ...|A12KSAYBWRE2EP|this book is one ...|[this, book, is, ...|[one, bibles, veg...|[one bibles, bibl...|\n",
      "|1,000 Vegetarian ...|I have used this ...|A14O63Q1W61YO5|i have used this ...|[i, have, used, t...|[used, cookbook, ...|[used cookbook, c...|\n",
      "|1,000 Vegetarian ...|In our house, we ...|A17FLJJOOO0M9V|in our house we h...|[in, our, house, ...|[house, several, ...|[house several, s...|\n",
      "|1,000 Vegetarian ...|I am pleasantly s...|A1BBJK1VE07KCV|i am pleasantly s...|[i, am, pleasantl...|[pleasantly, supr...|[pleasantly supri...|\n",
      "|1,000 Vegetarian ...|I own just about ...|A1FMR10ARQHGKF|i own just about ...|[i, own, just, ab...|[every, vegetaria...|[every vegetarian...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create word-level shingles (n-grams)\n",
    "def create_word_shingles(words, n=2):  \n",
    "    if len(words) < n:\n",
    "        return [\" \".join(words)]\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "# Define UDF\n",
    "shingle_udf = udf(lambda words: create_word_shingles(words, n=2), ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to generate shingles\n",
    "df_shingled = df_no_stop.withColumn(\"shingles\", shingle_udf(col(\"filtered_text\")))\n",
    "\n",
    "df_shingled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors (using hashing trick)\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=1 << 20)  # 2^20 features  \n",
    "featurized_data = hashing_tf.transform(df_shingled)   #each bigram gets hashed to an index between 0 and 1,048,575 and the value = freq of occurence (sparce vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|            features|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1,000 Vegetarian ...|This book is one ...|A12KSAYBWRE2EP|this book is one ...|[this, book, is, ...|[one, bibles, veg...|[one bibles, bibl...|(1048576,[19336,5...|\n",
      "|1,000 Vegetarian ...|I have used this ...|A14O63Q1W61YO5|i have used this ...|[i, have, used, t...|[used, cookbook, ...|[used cookbook, c...|(1048576,[10768,2...|\n",
      "|1,000 Vegetarian ...|In our house, we ...|A17FLJJOOO0M9V|in our house we h...|[in, our, house, ...|[house, several, ...|[house several, s...|(1048576,[48330,1...|\n",
      "|1,000 Vegetarian ...|I am pleasantly s...|A1BBJK1VE07KCV|i am pleasantly s...|[i, am, pleasantl...|[pleasantly, supr...|[pleasantly supri...|(1048576,[57465,7...|\n",
      "|1,000 Vegetarian ...|I own just about ...|A1FMR10ARQHGKF|i own just about ...|[i, own, just, ab...|[every, vegetaria...|[every vegetarian...|(1048576,[6810,37...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = featurized_data.withColumn(\"Review_ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHash LSH for Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinHash\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=6)\n",
    "model = mh.fit(df_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar reviews\n",
    "similar_reviews = model.approxSimilarityJoin(\n",
    "    df_with_id, df_with_id, 0.8, distCol=\"JaccardDistance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = similar_reviews.filter(col(\"datasetA.Review_ID\") != col(\"datasetB.Review_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only one direction of each pair\n",
    "filtered_df = filtered_df.filter(col(\"datasetA.review_ID\") < col(\"datasetB.review_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|Title_A                                                                                         |Title_B                                                                                         |review/text_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |review/text_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |JaccardDistance    |\n",
      "+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|High Fit, Low Fat Vegetarian                                                                    |High Fit, Low Fat Vegetarian                                                                    |I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The Eggplant Parmesan recipe is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too, I bought 10 as gifts for family members. Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The eggplant parmesan recipe (Vegetarian cookbook) is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too (I bought 10 as gifts for family members)! Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.08510638297872342|\n",
      "|The Biggest Loser Cookbook: More Than 125 Healthy, Delicious Recipes Adapted from NBC's Hit Show|The Biggest Loser Cookbook: More Than 125 Healthy, Delicious Recipes Adapted from NBC's Hit Show|I am an incredibly picky eater and usually if I get 1 or 2 recipes out of a cookbook that I buy I'm pretty lucky. I'm pretty \"plain jane\" when it comes to food. The Biggest Loser Cookbook is the perfect book for me because it takes a lot of those basic meals that I love and makes them healthier. I've even learned enough from the book that I can make adaptions and play around with the recipes to create even more great meals for myself and my family.Some of our favorite recipes from the book are the Open Faced Burritos, the Almost Fast Food Burger, the Picante Chicken, the roast beef sandwich, the breakfast sausages, and the turkey roll-ups. We haven't gotten through them all yet but we're working our way through the book.I would DEFINITELY recommend this book if you're a picky eater or you're one of those people who doesn't like a lot of extra stuff in their food. These are simple, plain recipes that taste like a million bucks.                                                                                                                        |This is a fun book that can help you eat healthier with a lot of good receipies.This review is from: The Biggest Loser Cookbook: More Than 125 Healthy, Delicious Recipes Adapted the tv Show Some of our favorite recipes from the book are the Open Faced Burritos, the Almost Fast Food Burger, the Picante Chicken, the roast beef sandwich, the breakfast sausages, and the turkey roll-ups. We haven't gotten through them all yet but we're working our way through the book. II am an incredibly picky eater and usually if I get 1 or 2 recipes out of a cookbook that I buy I'm pretty lucky. I'm pretty \"plain jane\" when it comes to food. The Biggest Loser Cookbook is the perfect book for me because it takes a lot of those basic meals that I love and makes them healthier. I've even learned enough from the book that I can make adaptions and play around with the recipes to create even more great meals for myself and my family.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |0.3980582524271845 |\n",
      "|The Frugal Gourmet Cooks Three Ancient Cuisines: China * Greece * Rome                          |The Frugal Gourmet on Our Immigrant Ancestors                                                   |This is an excellent cook book and this is the handy PB edition! It's full of great recipes and stories by a very talented cook and writer. This one focuses on 3 major influences in the culinary world. Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!|My title blurb is a funny quote I remembered, Jeff Smith spoke on his entertaining PBS show. Before 'The Food Network' we had the witty and talented 'Frugal Gourmet'. This book deals with some simplistic, yet very good classic old world dishes. Nothing fancy, just great traditional food!This is yet another excellent cook book by Jeff Smith! It's full of great recipes and stories by a very talented cook and writer. This one focuses on old world cooking. I have used many of these recipes and found them to be very good. Being a home grown cook myself and having had many of my grandmother's classic recipes handed down to me, I found this book to be very helpful in expanding my culinary taste buds.Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!|0.42361111111111116|\n",
      "+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3 = (\n",
    "    filtered_df\n",
    "    .orderBy(col(\"JaccardDistance\").asc())\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "top3.select(\n",
    "    col(\"datasetA.Title\").alias(\"Title_A\"),\n",
    "    col(\"datasetB.Title\").alias(\"Title_B\"),\n",
    "    col(\"datasetA.review/text\").alias(\"review/text_A\"),\n",
    "    col(\"datasetB.review/text\").alias(\"review/text_B\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out pairs with Jaccard distance of 0\n",
    "filtered_df = filtered_df.filter(col(\"JaccardDistance\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|Title_A                                                                                         |Title_B                                                                                         |review/text_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |review/text_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |JaccardDistance    |\n",
      "+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|High Fit, Low Fat Vegetarian                                                                    |High Fit, Low Fat Vegetarian                                                                    |I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The Eggplant Parmesan recipe is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too, I bought 10 as gifts for family members. Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both novice and experienced chefs. Using these cookbooks, I have learned how to eat incredibly healthy while serving really tasty meals! The eggplant parmesan recipe (Vegetarian cookbook) is now a requirement for potlucks I attend. I love it because it is so easy to make. The cookbooks make great gifts too (I bought 10 as gifts for family members)! Although I own many cookbooks, I do the majority of my cooking from the High Fit - Low Fat recipes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |0.08510638297872342|\n",
      "|The Biggest Loser Cookbook: More Than 125 Healthy, Delicious Recipes Adapted from NBC's Hit Show|The Biggest Loser Cookbook: More Than 125 Healthy, Delicious Recipes Adapted from NBC's Hit Show|I am an incredibly picky eater and usually if I get 1 or 2 recipes out of a cookbook that I buy I'm pretty lucky. I'm pretty \"plain jane\" when it comes to food. The Biggest Loser Cookbook is the perfect book for me because it takes a lot of those basic meals that I love and makes them healthier. I've even learned enough from the book that I can make adaptions and play around with the recipes to create even more great meals for myself and my family.Some of our favorite recipes from the book are the Open Faced Burritos, the Almost Fast Food Burger, the Picante Chicken, the roast beef sandwich, the breakfast sausages, and the turkey roll-ups. We haven't gotten through them all yet but we're working our way through the book.I would DEFINITELY recommend this book if you're a picky eater or you're one of those people who doesn't like a lot of extra stuff in their food. These are simple, plain recipes that taste like a million bucks.                                                                                                                        |This is a fun book that can help you eat healthier with a lot of good receipies.This review is from: The Biggest Loser Cookbook: More Than 125 Healthy, Delicious Recipes Adapted the tv Show Some of our favorite recipes from the book are the Open Faced Burritos, the Almost Fast Food Burger, the Picante Chicken, the roast beef sandwich, the breakfast sausages, and the turkey roll-ups. We haven't gotten through them all yet but we're working our way through the book. II am an incredibly picky eater and usually if I get 1 or 2 recipes out of a cookbook that I buy I'm pretty lucky. I'm pretty \"plain jane\" when it comes to food. The Biggest Loser Cookbook is the perfect book for me because it takes a lot of those basic meals that I love and makes them healthier. I've even learned enough from the book that I can make adaptions and play around with the recipes to create even more great meals for myself and my family.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |0.3980582524271845 |\n",
      "|The Frugal Gourmet Cooks Three Ancient Cuisines: China * Greece * Rome                          |The Frugal Gourmet on Our Immigrant Ancestors                                                   |This is an excellent cook book and this is the handy PB edition! It's full of great recipes and stories by a very talented cook and writer. This one focuses on 3 major influences in the culinary world. Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!|My title blurb is a funny quote I remembered, Jeff Smith spoke on his entertaining PBS show. Before 'The Food Network' we had the witty and talented 'Frugal Gourmet'. This book deals with some simplistic, yet very good classic old world dishes. Nothing fancy, just great traditional food!This is yet another excellent cook book by Jeff Smith! It's full of great recipes and stories by a very talented cook and writer. This one focuses on old world cooking. I have used many of these recipes and found them to be very good. Being a home grown cook myself and having had many of my grandmother's classic recipes handed down to me, I found this book to be very helpful in expanding my culinary taste buds.Jeff Smith entertained us for years on his PBS program 'The Frugal Gourmet'. Not only did he teach us many savory dishes, he also educated us. Not satisfied with just cooking delicious meals for his viewers, he would give detailed history lessons about the origins of the dish and made it all a lot of fun!This may be Mr. Smiths best cook book and it is a worthy edition to everyone's cook book library. I own and have read many, if not all of his cook books, not only for the man's knowledge of cooking, but his incredible wit! This guy was funny and I would have loved to have hung out and throw a few beers down with him.Unfortunately, this man had some very seriously bad press released about his personal life and well..... I am not one to spread rumors.....he seemed like a great guy and sadly he died before he was able to clear his name.R.I.P. Frugs!|0.42361111111111116|\n",
      "+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3 = (\n",
    "    filtered_df\n",
    "    .orderBy(col(\"JaccardDistance\").asc())\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "top3.select(\n",
    "    col(\"datasetA.Title\").alias(\"Title_A\"),\n",
    "    col(\"datasetB.Title\").alias(\"Title_B\"),\n",
    "    col(\"datasetA.review/text\").alias(\"review/text_A\"),\n",
    "    col(\"datasetB.review/text\").alias(\"review/text_B\"),\n",
    "    col(\"JaccardDistance\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + Cosine LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "# exclude rows where 'filtered_text' has less than 6 words\n",
    "featurized_data = featurized_data.filter(size(col(\"filtered_text\")) >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IDF to get TF-IDF vectors\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "df_tfidf = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               Title|         review/text|       User_id|          clean_text|               words|       filtered_text|            shingles|            features|      tfidf_features|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1,000 Vegetarian ...|This book is one ...|A12KSAYBWRE2EP|this book is one ...|[this, book, is, ...|[one, bibles, veg...|[one bibles, bibl...|(1048576,[19336,5...|(1048576,[19336,5...|\n",
      "|1,000 Vegetarian ...|I have used this ...|A14O63Q1W61YO5|i have used this ...|[i, have, used, t...|[used, cookbook, ...|[used cookbook, c...|(1048576,[10768,2...|(1048576,[10768,2...|\n",
      "|1,000 Vegetarian ...|In our house, we ...|A17FLJJOOO0M9V|in our house we h...|[in, our, house, ...|[house, several, ...|[house several, s...|(1048576,[48330,1...|(1048576,[48330,1...|\n",
      "|1,000 Vegetarian ...|I am pleasantly s...|A1BBJK1VE07KCV|i am pleasantly s...|[i, am, pleasantl...|[pleasantly, supr...|[pleasantly supri...|(1048576,[57465,7...|(1048576,[57465,7...|\n",
      "|1,000 Vegetarian ...|I own just about ...|A1FMR10ARQHGKF|i own just about ...|[i, own, just, ab...|[every, vegetaria...|[every vegetarian...|(1048576,[6810,37...|(1048576,[6810,37...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tfidf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_id = df_tfidf.withColumn(\"Review_ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH with Cosine-like projection (BucketedRandomProjectionLSH)\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.5, numHashTables=4)  #number of different ways you're looking for similarity.\n",
    "\n",
    "\n",
    "brp_model = brp.fit(df_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-join to find similar review pairs\n",
    "similar_pairs = brp_model.approxSimilarityJoin(df_with_id, df_with_id, threshold=5.0, distCol=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out duplicates and self-pairs\n",
    "similar_pairs_filtered = similar_pairs \\\n",
    "    .filter(col(\"datasetA.Review_ID\") < col(\"datasetB.Review_ID\")) \\\n",
    "    .filter(col(\"datasetA.Review_ID\") != col(\"datasetB.Review_ID\")) \\\n",
    "    .select(\"datasetA.Title\", \"datasetB.Title\", \"datasetA.User_id\", \"datasetB.User_id\", \"datasetA.review/text\", \"datasetB.review/text\", \"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o312.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 82) (100.65.113.22 executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msimilar_pairs_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m         },\n\u001b[0;32m    976\u001b[0m     )\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Anaconda3\\envs\\pyspark310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o312.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 82) (100.65.113.22 executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_filtered.orderBy(\"distance\").show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exlude pairs that are too similar (distance < 2.5)\n",
    "similar_pairs_filtered = similar_pairs_filtered.filter(col(\"distance\") > 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "|                                                                                         Title|                                                                                               Title|       User_id|       User_id|                                                                                         review/text|                                                                                         review/text|          distance|\n",
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "|                                                                  High Fit, Low Fat Vegetarian|                                                                        High Fit, Low Fat Vegetarian|A2JDVB1LKLN97N| AK9FPASRM79OX|I own both the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for both...|I own both of the High Fit - Low Fat cookbooks (regular and vegetarian). The books are good for b...|               2.0|\n",
      "|                                                       The All New All Purpose: Joy of Cooking|                                                All About Braising: The Art of Uncomplicated Cooking|A2PQTIF0WD7T3A| AO598PUMZ7CKU|                 My book arrived in a timely manner and as discribed. I would use this seller again.|Product arrived at a timely manner and the book is in good condition. Would purchase with the sam...|2.6457513110645907|\n",
      "|How to Grill: The Complete Illustrated Book of Barbecue Techniques, A Barbecue Bible! Cookbook|  This Can't Be Tofu!: 75 Recipes to Cook Something You Never Thought You Would--and Love Every Bite|A2ZPAJD0AJ0B66|A35NNPZP83H65F|                              this book is one of the best bq book on the market, i'm glad i got it.|            I'm so glad I got more recipes for tofu. It's so good for you. This is a wonderful book.|2.6457513110645907|\n",
      "|                      Jill Prescott's Ecole de Cuisine: Professional Cooking for the Home Chef|Saving Dinner the Low-Carb Way: Healthy Menus, Recipes, and the Shopping Lists That Will Keep the...|A1ZEHXQQLC9NB3|A191C036Y8W8CQ|This is a great cookbook. Everything I have made has been excellent. I have also given the book a...|           I have this and have also given it as gifts. For a busy family this is a great cool book.|2.6457513110645907|\n",
      "|                                                        Good Housekeeping Illustrated Cookbook|                                                       1000 Vegetarian Recipes From Around the World| A1ZGX2HV4L1JE| AYKWM682POY3J|                            this is best cookbook i have ever had. mine was falling apart. thank you|                     Whether you are vegetarian or not, this is the best cookbook I have ever owned.|2.6457513110645907|\n",
      "+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+--------------+--------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_filtered.orderBy(\"distance\").show(5, truncate=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
